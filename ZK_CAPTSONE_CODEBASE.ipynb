{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we actually predict market movements by analysing Reddit's /r/wallstreetbets?\n",
    "### Introduction:\n",
    "-In this project, I will be analyzing a large dataset from Reddit's /r/wallstreetbets of over 2.5m entries before analysing data scraped from Reddit using PushShift and Reddit's API PRAW. \n",
    "-We will be attempting to understand the effects of Reddit's /r/wallstreetbets comments on the market as a whole.\n",
    "-We will also be scraping comments from /r/wallstreetbets Daily Discussion Threads because it has the most reliable and useful (not just spam) information from the users. \n",
    "-We will overlay the graphs with the price and volume trends of the filtered tickers and market indexes. \n",
    "-Lastly, we'll shift the close price of the Dow Jones by 1, 2 and 3 days and see if we can predict market movements based on the text we've analyzed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First thing, let's load in our data from json\n",
    "\n",
    "file_path = 'YOUR_FILE_PATH'\n",
    "\n",
    "empty = []\n",
    "for line in open(file_path, 'r'):\n",
    "    empty.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cast it to a dataframe\n",
    "\n",
    "df = pd.DataFrame(empty)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin cleaning the data\n",
    "#Drop columns that serve no purpose in our analysis\n",
    "\n",
    "df['date_created'] = pd.to_datetime(df['created_utc'].astype(int), unit='s')\n",
    "df.drop(columns=['created_utc','archived', 'controversiality','retrieved_on','downs','ups','subreddit'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date_created'].dt.date\n",
    "df.drop(columns=['date_created','gilded','link_id','id', \n",
    "                 'score_hidden', 'name', 'author', 'subreddit_id', 'parent_id', \n",
    "                 'author_flair_text', 'author_flair_css_class','distinguished'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all deleted values\n",
    "\n",
    "df = df.drop(df[df['body'].map(lambda x: str(x)==\"[deleted]\")].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use VADER on the body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound_scores=[]\n",
    "positive_scores=[]\n",
    "negative_scores=[]\n",
    "\n",
    "for item in tqdm(df['body']):\n",
    "    positive_score=0\n",
    "    negative_score=0\n",
    "    compound_score=0\n",
    "    try:\n",
    "        positive_score=positive_score+analyser.polarity_scores(item)['pos']\n",
    "        negative_score=negative_score+analyser.polarity_scores(item)['neg']\n",
    "        compound_score=compound_score+analyser.polarity_scores(item)['compound']\n",
    "    except TypeError:\n",
    "        sentiment_score=0\n",
    "    \n",
    "    positive_scores.append(positive_score)\n",
    "    negative_scores.append(negative_score)\n",
    "    compound_scores.append(compound_score)\n",
    "    \n",
    "    \n",
    "df['compound_score'] = compound_scores\n",
    "df['positive_score'] = positive_scores\n",
    "df['negative_score'] = negative_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time to standardize the scoring column as a new column. \n",
    "## Create score times the sentiment score column as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "df['standardized_upvotes'] = scaler.fit_transform(df[['score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create score multiplied by upvote columns\n",
    "\n",
    "df['compound_score_upvotes'] = df.compound_score * df.standardized_upvotes\n",
    "df['positive_score_upvotes'] = df.positive_score * df.standardized_upvotes\n",
    "df['negative_score_upvotes'] = df.negative_score * df.standardized_upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index and fix up the date\n",
    "\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df = df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up all the sentiments along the date\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time to grab market data from Yahoo finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "dataDJI = yf.download(\"DJI\", start=\"2012-04-11\", end=\"2018-10-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, DJI, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plot the dataframe to check the graphs and relationships\n",
    "\n",
    "merged_df.plot(secondary_y='Close', figsize=(20,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the Close column\n",
    "\n",
    "merged_df['standardized_close'] = scaler.fit_transform(merged_df[['Close']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up our target variable 'up'. 1 for positive change, 0 for negative.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['up'] = (merged_df.Close.diff() > 0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 3 new features, the Close column shifted 1, 2 and 3 times\n",
    "\n",
    "merged_df['Close_shift_1'] = merged_df.Close.shift(1)\n",
    "merged_df['Close_shift_2'] = merged_df.Close.shift(2)\n",
    "merged_df['Close_shift_3'] = merged_df.Close.shift(3)\n",
    "\n",
    "X = merged_df[['up','Close', 'Close_shift_1', 'Close_shift_2', 'Close_shift_3', \n",
    "                   'standardized_close','negative_score_upvotes','positive_score_upvotes', \n",
    "                   'compound_score_upvotes', 'standardized_upvotes', 'negative_score', 'positive_score', 'compound_score', 'upvotes']].copy()\n",
    "\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up the test variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.pop('up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-test split the data by index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1405\n",
    "X_train, y_train = X[:n], y[:n]\n",
    "X_test, y_test = X[n:], y[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeriesSplit(n_splits=7)\n",
    "\n",
    "splits = [(tr, te) for (tr, te) in ts.split(X_train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time to begin modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10000)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=ts)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional step to clean up the body text, run Tf-IDF on the cleaned text and add them as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use spaCy to clean up the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up the text\n",
    "\n",
    "def spacy_cleaner(text):\n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(text)\n",
    "    apostrophe_handled = re.sub(\"â€™\", \"'\", decoded)\n",
    "    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            if t.lemma_ == '-PRON-':\n",
    "                final_tokens.append(str(t))\n",
    "            else:\n",
    "                sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\n",
    "                if len(sc_removed) > 1:\n",
    "                    final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = tqdm([spacy_cleaner(t) for t in df.body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add it back to the dataframe \n",
    "\n",
    "df['cleaned_text'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dtype to string\n",
    "\n",
    "df.cleaned_text = df.cleaned_text.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF on the cleaned text\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "tfed_train = tvec.fit_transform(df.cleaned_text[:1405])\n",
    "tfed_test = tvec.transform(df.cleaned_text[1405:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale my X_train and X_test\n",
    "\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sparse matrix for my X_train and X_test so I can combine it with the Tf-idf'd features\n",
    "\n",
    "X_sparse = scipy.sparse.csr_matrix(X_train.values)\n",
    "X_sparse_test = scipy.sparse.csr_matrix(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New X_train and X_test \n",
    "\n",
    "X_train = scipy.sparse.hstack((X_sparse, tfed_train))\n",
    "X_test = scipy.sparse.hstack((X_sparse_test, tfed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the features again\n",
    "\n",
    "ts = TimeSeriesSplit(n_splits=7)\n",
    "\n",
    "splits = [(tr, te) for (tr, te) in ts.split(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the baseline\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a RandomForest again \n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10000)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=ts)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Scraping Reddit Using PushShift and PRAW to analyse market returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to scrape data from Reddit using Pushshift and PRAW. Then we'll run similar analysis as above with some additional features catered to our new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get data from pushshift api\n",
    "\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "#get relevant data from data extracted using previous function\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = [subm['id'], subm['title'], subm['url'], datetime.datetime.fromtimestamp(subm['created_utc']).date()]\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"\n",
    "    subData.append(flair)\n",
    "    subStats.append(subData)\n",
    "    \n",
    "#Subreddit to query\n",
    "sub='wallstreetbets'\n",
    "\n",
    "#before and after dates\n",
    "before = \"1596240000\" #August 1 2020\n",
    "after = \"1541030400\" #Nov 1 2018\n",
    "\n",
    "#query string\n",
    "query = \"Daily Discussion Thread\"\n",
    "subCount = 0\n",
    "subStats = []\n",
    "\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'after' date up until before date\n",
    "\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    \n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "    \n",
    "\n",
    "#organize data into dataframe\n",
    "data={}\n",
    "ids=[]\n",
    "titles=[]\n",
    "urls=[]\n",
    "dates=[]\n",
    "flairs=[]\n",
    "for stat in subStats:\n",
    "    ids.append(stat[0])\n",
    "    titles.append(stat[1])\n",
    "    urls.append(stat[2])\n",
    "    dates.append(stat[3])\n",
    "    flairs.append(stat[4])\n",
    "data['id']=ids\n",
    "data['title']=titles\n",
    "data['url']=urls\n",
    "data['date']=dates\n",
    "data['flair']=flairs\n",
    "df_1=pd.DataFrame(data)\n",
    "df_1=df_1[df_1['flair']=='Daily Discussion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to reddit api\n",
    "reddit = praw.Reddit(client_id=\"YOUR_ID\", client_secret=\"YOUR_SECRET\", user_agent=\"USER_AGENT\")\n",
    "\n",
    "#collect comments using praw\n",
    "comments_by_day=[]\n",
    "for url in df_1['url'].tolist():\n",
    "    try:\n",
    "        submission = reddit.submission(url=url)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        comments=list([(comment.body) for comment in submission.comments])\n",
    "    except:\n",
    "        comments=None\n",
    "    comments_by_day.append(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the comments as a column\n",
    "\n",
    "df_1['comments'] = comments_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This gives me the overall vibe of the day's comments. This can be used to analyze the full market (index) rather than individual tickers. \n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound_scores=[]\n",
    "positive_scores=[]\n",
    "negative_scores=[]\n",
    "\n",
    "for comments in tqdm(comments_by_day):\n",
    "    positive_score=0\n",
    "    negative_score=0\n",
    "    compound_score=0\n",
    "    try:\n",
    "        for comment in comments:\n",
    "            positive_score=positive_score+analyser.polarity_scores(comment)['pos']\n",
    "            negative_score=negative_score+analyser.polarity_scores(comment)['neg']\n",
    "            compound_score=compound_score+analyser.polarity_scores(comment)['compound']\n",
    "    except TypeError:\n",
    "        positive_score=0\n",
    "        negative_score=0\n",
    "        compound_score=0\n",
    "    \n",
    "    positive_scores.append(positive_score)\n",
    "    negative_scores.append(negative_score)\n",
    "    compound_scores.append(compound_score)\n",
    "    \n",
    "df_1['compound_score'] = compound_scores\n",
    "df_1['positive_score'] = positive_scores\n",
    "df_1['negative_score'] = negative_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we need to get the titles of posts from reddit and just run a bull/bear analysis on it. \n",
    "###### No need for praw here again, we'll just use pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use pushshift again to get the titles\n",
    "\n",
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = [subm['id'], subm['title'], subm['url'], datetime.datetime.fromtimestamp(subm['created_utc']).date()]\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"\n",
    "    subData.append(flair)\n",
    "    subStats.append(subData)\n",
    "    \n",
    "#Subreddit to query\n",
    "sub='wallstreetbets'\n",
    "#before and after dates\n",
    "before = \"1596240000\" #August 1 2020\n",
    "after = \"1541030400\" #Nov 1 2018\n",
    "query = ''\n",
    "subCount = 0\n",
    "subStats = []\n",
    "\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    try:\n",
    "        data = getPushshiftData(query, after, before, sub)\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "data={}\n",
    "ids=[]\n",
    "titles=[]\n",
    "urls=[]\n",
    "dates=[]\n",
    "flairs=[]\n",
    "for stat in subStats:\n",
    "    ids.append(stat[0])\n",
    "    titles.append(stat[1])\n",
    "    urls.append(stat[2])\n",
    "    dates.append(stat[3])\n",
    "    flairs.append(stat[4])\n",
    "data['id']=ids\n",
    "data['title']=titles\n",
    "data['url']=urls\n",
    "data['date']=dates\n",
    "data['flair']=flairs\n",
    "df_2=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create a list of bullish and bearish sentiments to analyze the comments\n",
    "\n",
    "titles=df_2['title'].tolist()\n",
    "titles=list([(title.lower()) for title in titles])\n",
    "\n",
    "bull_words=['call', 'long', 'all in', 'moon', 'going up', 'rocket', 'buy', 'long term', 'green']\n",
    "bear_words=['put', 'short', 'going down', 'drop', 'bear', 'sell', 'red', 'sell', 'leave']\n",
    "\n",
    "bull_scores=[]\n",
    "bear_scores=[]\n",
    "for title in titles:\n",
    "    bull=False\n",
    "    bear=False\n",
    "    for word in bull_words:\n",
    "        if word in title:\n",
    "            bull=True\n",
    "    if re.findall(r'(\\b\\d{1,4}[c]\\b)|(\\b\\d{1,4}[ ][c]\\b)', title):\n",
    "            bull=True\n",
    "            \n",
    "    for word in bear_words:\n",
    "        if word in title:\n",
    "            bear=True\n",
    "    if re.findall(r'(\\b\\d{1,4}[p]\\b)|(\\b\\d{1,4}[ ][p]\\b)', title):\n",
    "            bear=True\n",
    "            \n",
    "    if bull==True and bear==True:\n",
    "        bull_scores.append(0)\n",
    "        bear_scores.append(0)\n",
    "    if bull==False and bear==False:\n",
    "        bull_scores.append(0)\n",
    "        bear_scores.append(0)\n",
    "    if bull==True and bear==False:\n",
    "        bull_scores.append(1)\n",
    "        bear_scores.append(0)\n",
    "    if bull==False and bear==True:\n",
    "        bull_scores.append(0)\n",
    "        bear_scores.append(1)\n",
    "        \n",
    "df_2['bull_score']=bull_scores\n",
    "df_2['bear_score']=bear_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces=[]\n",
    "flairs=df_2['flair'].tolist()\n",
    "for n in range(len(flairs)):\n",
    "    if flairs[n]=='DD' or flairs[n]=='Discussion' or flairs[n]=='YOLO' or flairs[n]=='Fundamentals' or flairs[n]=='Stocks':\n",
    "        indeces.append(n)\n",
    "df_2=df_2.iloc[indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize scores using total scores for day\n",
    "\n",
    "scores_df=df_2.groupby('date').sum()\n",
    "scores_df['bull_score']=scores_df['bull_score']/df_2.groupby('date').count()['bull_score']\n",
    "scores_df['bear_score']=scores_df['bear_score']/df_2.groupby('date').count()['bear_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This time, instead of the Dow Jones price, gather the SPY price (tracks the S&P 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "spy_price = yf.download(\"SPY\", start=\"2018-11-01\", end=\"2020-08-01\")\n",
    "spy_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=yf.download(\"SPY\", start='2018-11-01')\n",
    "df_2=df_2.loc[:'2020-08-01']\n",
    "\n",
    "bull_vals=[]\n",
    "bear_vals=[]\n",
    "\n",
    "for date in df_2.index.tolist():\n",
    "    bull_vals.append(float(scores_df.loc[date.date()]['bull_score']))\n",
    "    bear_vals.append(float(scores_df.loc[date.date()]['bear_score']))\n",
    "        \n",
    "df_2['bull_score']=bull_vals\n",
    "df_2['bear_score']=bear_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph the bull score with the SPY price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[['Close', 'bull_score']].plot(secondary_y='bull_score', color=['b','c'], figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph the bear score with the SPY price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[['Close', 'bear_score']].plot(secondary_y='bear_score', color=['b','y'], figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph the VADER compound score and the SPY price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['Close', 'compound_score']].plot(secondary_y='compound_score', color=['b','r'], figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### At this point, we can see a trend of some sort but we need to transform the graphs using Fourier transformation to create new features and visualize trends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fourier transform\n",
    "\n",
    "close_fft = np.fft.fft(np.asarray(df_2['bull_score'].tolist()))\n",
    "fft_df = pd.DataFrame({'fft':close_fft})\n",
    "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
    "fft_list = np.asarray(fft_df['fft'].tolist())\n",
    "\n",
    "for num_ in [10, 30]:\n",
    "    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n",
    "    df_2['fourier bull '+str(num_)]=np.fft.ifft(fft_list_m10)\n",
    "\n",
    "close_fft = np.fft.fft(np.asarray(df_2['bear_score'].tolist()))\n",
    "fft_df = pd.DataFrame({'fft':close_fft})\n",
    "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
    "fft_list = np.asarray(fft_df['fft'].tolist())\n",
    "\n",
    "for num_ in [10, 30]:\n",
    "    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n",
    "    df_2['fourier bear '+str(num_)]=np.fft.ifft(fft_list_m10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the Fourier bull scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[['bull_score', 'fourier bull 10', 'fourier bull 30']].plot(color=['y','b','r'], figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the bear scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[['bear_score', 'fourier bear 10', 'fourier bear 30']].plot(color=['y','k','m'],figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc= MinMaxScaler(feature_range=(0,1))\n",
    "df_2['norm_price']=sc.fit_transform(df_2['Close'].to_numpy().reshape(-1, 1))\n",
    "df_2['Close_log']=np.log(df_2['Close']/df_2['Close'].shift(1))\n",
    "df_2['norm_bull']=sc.fit_transform(df_2['bull_score'].to_numpy().reshape(-1, 1))\n",
    "df_2['norm_bear']=sc.fit_transform(df_2['bear_score'].to_numpy().reshape(-1, 1))\n",
    "df_2['norm_fourier_bull_10']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_2['fourier bull 10'].to_numpy()])).reshape(-1, 1))\n",
    "df_2['norm_fourier_bear_10']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_2['fourier bear 10'].to_numpy()])).reshape(-1, 1))\n",
    "df_2['norm_fourier_bull_30']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_2['fourier bull 30'].to_numpy()])).reshape(-1, 1))\n",
    "df_2['norm_fourier_bear_30']=sc.fit_transform(np.asarray(list([(float(x)) for x in df_2['fourier bear 30'].to_numpy()])).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the bull score with the new normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[['norm_price', 'norm_fourier_bull_10', 'norm_fourier_bull_30']].plot(color=['k','c','m'], figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create another merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_1, spy_price, how='inner', left_index=True, right_index=True)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the sentiment scroes as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fourier transform the sentiment score\n",
    "\n",
    "close_fft = np.fft.fft(np.asarray(merged_df['compound_score'].tolist()))\n",
    "fft_df = pd.DataFrame({'fft':close_fft})\n",
    "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
    "fft_list = np.asarray(fft_df['fft'].tolist())\n",
    "\n",
    "for num_ in [5, 10, 15, 20]:\n",
    "    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n",
    "    merged_df['fourier '+str(num_)]=np.fft.ifft(fft_list_m10)\n",
    "    \n",
    "merged_df[['compound_score', 'fourier 5', 'fourier 10', 'fourier 15', 'fourier 20']].plot(color=['y','c','m','g','k'],figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()\n",
    "df_2['date'] = pd.to_datetime(df_2['Date'], format='%Y-%m-%d')\n",
    "df_2.set_index('date', inplace=True)\n",
    "df_2.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create the Robinhood dataframe\n",
    "\n",
    "robinhood = pd.read_csv('FILEPATH_TO_ROBINTRACK_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robinhood['clean_date'] = pd.to_datetime(robinhood['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robinhood.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robinhood.set_index('clean_date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final = pd.merge(df_2, robinhood, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New temporary dataframe \n",
    "\n",
    "new_df = merged_df.drop(columns=['id','title','url','flair','fourier 5','fourier 10', 'fourier 15', 'fourier 20'])\n",
    "\n",
    "super_final = pd.merge(merged_final, new_df, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin setting up our modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable 'up' as defined before\n",
    "\n",
    "super_final['up'] = ((super_final).Close.diff() > 0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_final['Close_shift_1'] = super_final.Close.shift(1)\n",
    "super_final['Close_shift_2'] = super_final.Close.shift(2)\n",
    "super_final['Close_shift_3'] = super_final.Close.shift(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_final.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target variable\n",
    "\n",
    "y = super_final.up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_final.rename(columns={'norm_price_y':'norm_price'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = super_final[['bull_score', 'bear_score', 'Close_log', 'norm_bull',\n",
    "       'norm_bear', 'norm_fourier_bull_10', 'norm_fourier_bear_10',\n",
    "       'norm_fourier_bull_30', 'norm_fourier_bear_30', 'users_holding',\n",
    "       'compound_score', 'positive_score', 'negative_score', 'Open', 'High',\n",
    "       'Low', 'Close', 'Adj Close', 'Volume', 'norm_price', 'Close log',\n",
    "       'norm_sentiment', 'norm_fourier5', 'norm_fourier10', 'norm_fourier15',\n",
    "       'norm_fourier20', 'Close_shift_1', 'Close_shift_2',\n",
    "       'Close_shift_3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-test split at our index again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 180\n",
    "X_train, y_train = X[:n], y[:n]\n",
    "X_test, y_test = X[n:], y[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit\n",
    "\n",
    "ts = TimeSeriesSplit(n_splits=7)\n",
    "\n",
    "splits = [(tr, te) for (tr, te) in ts.split(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's start modelling by using this fantastic function\n",
    "This essentially GridSearches across LogisticRegression, DecisionTreeClassifier, RandomForest, KNeighborClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Big Boy Testing\n",
    "\n",
    "def test_models(models, X=None, y=None, split_data=True, scaler_type=StandardScaler()):    \n",
    "    results = {}\n",
    "    fitted_models = {}\n",
    "    train_test_sets = {}\n",
    "    for i in models:\n",
    "        print(f\"{i} model is currently running...\")\n",
    "        # split data\n",
    "        if split_data:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = X[0], X[1], y[0], y[1]\n",
    "            #print(X_train, X_test, y_train, y_test)\n",
    "        train_test_sets[i] = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n",
    "        # scale X if relevant\n",
    "        if scaler_type is not None:\n",
    "            scaler = scaler_type\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        # fit models\n",
    "        train_model = models[i].fit(X_train, y_train)\n",
    "        print(f'{i} model fitted successfully.')\n",
    "        results[i] = pd.DataFrame(train_model.cv_results_)\n",
    "        fitted_models[i] = train_model\n",
    "        \n",
    "    return results, fitted_models, train_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_params = {'C': np.logspace(-5, 5, 15),\n",
    "          'penalty': ['l1', 'l2'],\n",
    "          'fit_intercept': [True, False],\n",
    "          'max_iter': [100000],\n",
    "          'verbose': [1],\n",
    "          'random_state': [7]}\n",
    "\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 10, 15, 20, 25],\n",
    "        }\n",
    "\n",
    "cart_params = {\n",
    "    'max_depth': list(range(1, 21))+[None],\n",
    "    'max_features': [None, 1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4, 5, 10, 15, 20, 25, 30, 40, 50],\n",
    "    'ccp_alpha': [0, 0.001, 0.005, 0.01],\n",
    "            }\n",
    "\n",
    "random_params = {\n",
    "    'n_estimators': [5, 10, 25, 40],\n",
    "    'max_depth': [3, 5, 9],\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    'cv': ts,\n",
    "    'n_jobs':2,\n",
    "    'return_train_score':True,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'logit': GridSearchCV(LogisticRegression(), param_grid=logit_params, **kwargs),\n",
    "    'cart': GridSearchCV(DecisionTreeClassifier(), param_grid=cart_params, **kwargs),\n",
    "    'knn' : GridSearchCV(KNeighborsClassifier(), param_grid=knn_params, **kwargs),\n",
    "    'random_forest': GridSearchCV(RandomForestClassifier(), param_grid=random_params, **kwargs)\n",
    "}\n",
    "\n",
    "X, y = (X_train, X_test), (y_train, y_test)\n",
    "results, gs, train_test_sets = test_models(models, X=X, y=y, split_data=False, scaler_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in gs.items():\n",
    "    if k == 'Logit':\n",
    "        print(k)\n",
    "        print('Best Parameters:')\n",
    "        print(v.best_params_)\n",
    "        print('Best estimator mean cross validated training score:')\n",
    "        print(v.best_score_)\n",
    "        print('Best estimator score on the full training set:')\n",
    "        print(v.score(train_test_sets[k]['X_train'], train_test_sets[k]['y_train']))\n",
    "        print('Best estimator score on the test set:')\n",
    "        print(v.score(train_test_sets[k]['X_test'], train_test_sets[k]['y_test']))\n",
    "        print('Best estimator coefficients:')\n",
    "        logr_model1_coefs = pd.DataFrame(list(zip(Xc.columns, v.best_estimator_.coef_[0])), columns=['feature', 'coef']).sort_values(by='coef')\n",
    "        logr_model1_coefs\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print(k)\n",
    "        print('Best Parameters:')\n",
    "        print(v.best_params_)\n",
    "        print('Best estimator mean cross validated training score:')\n",
    "        print(v.best_score_)\n",
    "        print('Best estimator score on the full training set:')\n",
    "        print(v.score(train_test_sets[k]['X_train'], train_test_sets[k]['y_train']))\n",
    "        print('Best estimator score on the test set:')\n",
    "        print(v.score(train_test_sets[k]['X_test'], train_test_sets[k]['y_test']))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LogisticRegression was our best model. So now let's extract the confusion matrix, ROC/AUC curves and classification reports for that model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try some regression models\n",
    "X = super_final[['bull_score', 'bear_score', 'norm_bull',\n",
    "       'norm_bear', 'norm_fourier_bull_10', 'norm_fourier_bear_10',\n",
    "       'norm_fourier_bull_30', 'norm_fourier_bear_30', 'users_holding',\n",
    "       'compound_score', 'positive_score', 'negative_score', \n",
    "       'norm_sentiment', 'norm_fourier5', 'norm_fourier10', 'norm_fourier15',\n",
    "       'norm_fourier20', 'Close_shift_1', 'Close_shift_2',\n",
    "       'Close_shift_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = super_final.up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 180\n",
    "X_train, y_train = X[:n], y[:n]\n",
    "X_test, y_test = X[n:], y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "splits = [(tr, te) for (tr, te) in ts.split(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "          'C': np.logspace(-4, 4, 10),\n",
    "          'penalty': ['l2'],\n",
    "          'fit_intercept': [True, False],\n",
    "          'max_iter': [100000],\n",
    "          'verbose': [1]}\n",
    "\n",
    "gs = GridSearchCV(estimator=model,\n",
    "                  param_grid=params,\n",
    "                  cv=ts,\n",
    "                  scoring='accuracy',\n",
    "                  return_train_score=True)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# extract the grid search results\n",
    "\n",
    "print('Best Parameters:')\n",
    "print(gs.best_params_)\n",
    "print('Best estimator C:')\n",
    "print(gs.best_estimator_.C)\n",
    "print('Best estimator mean cross validated training score:')\n",
    "print(gs.best_score_)\n",
    "print('Best estimator score on the full training set:')\n",
    "print(gs.score(X_train, y_train))\n",
    "print('Best estimator score on the test set:')\n",
    "print(gs.score(X_test, y_test))\n",
    "print('Best estimator coefficients:')\n",
    "print(gs.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report\n",
    "predictions_train = gs.predict(X_train)\n",
    "predictions_test = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_train = gs.predict_proba(X_train)\n",
    "probabilities_test = gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "skplt.metrics.plot_roc(y_test, probabilities_test, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "skplt.metrics.plot_confusion_matrix(y_test, predictions_test, cmap='Wistia',labels=[1, 0], figsize=(6, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather feature importance\n",
    "best_features = pd.DataFrame(gs.best_estimator_.coef_, columns=X.columns).transpose()\n",
    "best_features.rename(columns={0:'Feature Importance'}, inplace=True)\n",
    "best_features.sort_values(by='Feature Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
